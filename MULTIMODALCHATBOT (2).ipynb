{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L8w3LqrUW5h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c16de49-809b-4234-c903-b3fd0326b96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.20.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.5.40)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%pip install transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler"
      ],
      "metadata": {
        "id": "BlTFistikMON"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"/content/cleaned_first_100_issues.csv\")\n",
        "\n",
        "# Print the column headings\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp6qyE9QrzFd",
        "outputId": "9e747177-8e6e-446c-9c90-eb96bbb2f156"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['title', 'labels', 'body', 'issue_url', 'repository_url', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"Enter your request: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use 'HEY TAKE ME FROM [start location] TO [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description:\", route_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "iDdFZ0pMsyqz",
        "outputId": "a872fa71-d3a7-48aa-d8e4-63b13696746d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-451e5aaec94a>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def record_audio():\n",
        "    # Record audio using the microphone\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Speak now...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio, language='en-US')  # Adjust language as needed\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand audio.\")\n",
        "        return \"\"\n",
        "    except sr.RequestError:\n",
        "        print(\"Sorry, there was a problem with the speech recognition service.\")\n",
        "        return \"\"\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"Enter your request (text/audio): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    if user_input.startswith('audio'):\n",
        "        # Record audio\n",
        "        user_input = record_audio()\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Invalid input format. Please use 'HEY TAKE ME FROM [start location] TO [end location]'.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description:\", route_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "lqWMVF70thcT",
        "outputId": "6f768d18-c56b-4966-a4f8-2b4559177abe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter your request (text/audio): text\n",
            "Invalid input format. Please use 'HEY TAKE ME FROM [start location] TO [end location]'.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e0a05dd1fab9>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request (text/audio): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import wave\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from googletrans import Translator  # For multilingual input\n",
        "import openai\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"text-davinci-003\",  # Consider using a more descriptive model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def record_audio_sounddevice():\n",
        "    duration = 5  # Adjust recording duration as needed\n",
        "    fs = 44100  # Sample rate\n",
        "    print(\"Speak now (recording for\", duration, \"seconds)...\")\n",
        "\n",
        "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
        "    sd.wait()\n",
        "\n",
        "    print(\"Recording stopped.\")\n",
        "\n",
        "    # Save the recorded audio to a WAV file\n",
        "    wav.write('recorded_audio.wav', fs, recording)\n",
        "\n",
        "def transcribe_audio_whisper(audio_file_path):\n",
        "    # Placeholder for Whisper API integration\n",
        "    # Replace with your Whisper API integration logic\n",
        "    # For example, using requests library to send audio data to Whisper API\n",
        "    # and getting back the transcribed text\n",
        "\n",
        "    # For the sake of example, assume audio_file_path contains audio data\n",
        "    # and whisper_api_key is your Whisper API key\n",
        "    whisper_api_key = \"\"\n",
        "\n",
        "    # Example placeholder code (replace with actual Whisper API integration)\n",
        "    with open(audio_file_path, 'rb') as f:\n",
        "        audio_data = f.read()\n",
        "\n",
        "    # Placeholder for Whisper API call\n",
        "    # Example placeholder response:\n",
        "    transcribed_text = \"This is where your transcribed text would be.\"\n",
        "\n",
        "    return transcribed_text\n",
        "\n",
        "def translate_text(text, target_language=\"en\"):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest=target_language).text\n",
        "    return translated_text\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"Enter your request (text/audio [language: en|es|fr...]): \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if user_input.startswith('audio'):\n",
        "            # Record audio using sounddevice\n",
        "            record_audio_sounddevice()\n",
        "\n",
        "            # Placeholder for Whisper API integration (currently using transcribe_audio_whisper placeholder)\n",
        "            audio_file_path = \"recorded_audio.wav\"\n",
        "            transcribed_text = transcribe_audio_whisper(audio_file_path)\n",
        "\n",
        "            if not transcribed_text:\n",
        "                print(\"Could not transcribe audio. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            user_input = transcribed_text\n",
        "\n",
        "        # Optional: Translate text if language specified\n",
        "        if language_code:\n",
        "            user_input = translate_text(user_input, language_code)\n",
        "\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "jcBmJAuXvpUh",
        "outputId": "52d2c786-4596-4d28-de7c-055aa8e49e95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "PortAudio library not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-187f581aecfb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PortAudio library not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_libname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: PortAudio library not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import os\n",
        "import re\n",
        "import openai\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def translate_text(text, target_language=\"en\"):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest=target_language).text\n",
        "    return translated_text\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"Enter your request (text/audio [language: en|es|fr...]): \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if user_input.startswith('audio'):\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            # Optional: Translate text if language specified\n",
        "            if language_code:\n",
        "                user_input = translate_text(user_input, language_code)\n",
        "\n",
        "            # Extract locations from user input\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            # Handle text input directly\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Optional: Translate text if language specified\n",
        "            if language_code:\n",
        "                user_input = translate_text(user_input, language_code)\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbzoxvOiyjgr",
        "outputId": "e07dc117-2261-4277-b328-d4bcb88ef32b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter your request (text/audio [language: en|es|fr...]): EXIT\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bnRZx0qvzARE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import os\n",
        "import re\n",
        "import openai\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract locations from input\n",
        "    pattern = re.compile(r\"FROM\\s+(.+?)\\s+TO\\s+(.+)\", re.IGNORECASE)\n",
        "    match = pattern.search(user_input)\n",
        "    if match:\n",
        "        start_location = match.group(1).strip()\n",
        "        end_location = match.group(2).strip()\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def translate_text(text, target_language=\"en\"):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest=target_language).text\n",
        "    return translated_text\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            # Optional: Translate text if language specified\n",
        "            if language_code:\n",
        "                user_input = translate_text(user_input, language_code)\n",
        "\n",
        "            # Extract locations from user input\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            # Handle text input directly\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Optional: Translate text if language specified\n",
        "            if language_code:\n",
        "                user_input = translate_text(user_input, language_code)\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "O12Xcr0TzAck",
        "outputId": "ff05c734-1493-442b-ad96-ecdc990d383e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request:      \n",
            "Could not extract start and end locations from input. Please try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-82226a3a9923>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-82226a3a9923>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from googletrans import Translator\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Initialize SpaCy model for English\n",
        "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Detect language of user input\n",
        "    translator = Translator()\n",
        "    detected_lang = translator.detect(user_input).lang\n",
        "\n",
        "    # Translate input to English if necessary\n",
        "    if detected_lang != 'en':\n",
        "        user_input = translator.translate(user_input, src=detected_lang, dest='en').text\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp_en(user_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def translate_text(text, target_language=\"en\"):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest=target_language).text\n",
        "    return translated_text\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if language_code:\n",
        "            # Translate user input to English if necessary\n",
        "            user_input = translate_text(user_input, \"en\")\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "        else:\n",
        "            # Handle text input directly\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "OSpj8WVu0F0e",
        "outputId": "7ec432ef-72f7-430e-a909-134c32e3dfbc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:      \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'group'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9fa0254c0622>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-9fa0254c0622>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# Handle text input directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not extract start and end locations from input. Please try again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9fa0254c0622>\u001b[0m in \u001b[0;36mextract_locations\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Detect language of user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mdetected_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Translate input to English if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# actual source language that will be recognized by Google Translator when the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Initialize SpaCy model for English\n",
        "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Translate input to English using OpenAI\n",
        "    translated_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp_en(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        user_input = input(\"Enter your request: \")\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if language_code:\n",
        "            # Translate user input to English if necessary\n",
        "            user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "        else:\n",
        "            # Handle text input directly\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "itOo_kX000Dk",
        "outputId": "78ee6880-617d-4ad7-beed-025ecb2556ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: audio\n",
            "Enter your request:      \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var recordButton = document.createElement('button');\n",
              "recordButton.innerText = 'Press to start recording';\n",
              "document.body.appendChild(recordButton);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "    gumStream = stream;\n",
              "    var options = {\n",
              "        mimeType: 'audio/webm;codecs=opus'\n",
              "    };            \n",
              "    recorder = new MediaRecorder(stream, options);\n",
              "    recorder.ondataavailable = function(e) {            \n",
              "        var url = URL.createObjectURL(e.data);\n",
              "        var preview = document.createElement('audio');\n",
              "        preview.controls = true;\n",
              "        preview.src = url;\n",
              "        document.body.appendChild(preview);\n",
              "\n",
              "        reader = new FileReader();\n",
              "        reader.readAsDataURL(e.data); \n",
              "        reader.onloadend = function() {\n",
              "            base64data = reader.result;\n",
              "        }\n",
              "    };\n",
              "    recorder.start();\n",
              "};\n",
              "\n",
              "recordButton.onclick = function() {\n",
              "    if (recorder && recorder.state == \"recording\") {\n",
              "        recorder.stop();\n",
              "        gumStream.getAudioTracks()[0].stop();\n",
              "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
              "    }\n",
              "};\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-36708923a777>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-36708923a777>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Evaluate JavaScript to get base64 encoded audio data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base64data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# Convert from webm to wav format using ffmpeg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "function startRecording() {\n",
        "    var recordButton = document.createElement('button');\n",
        "    recordButton.innerText = 'Recording... Click to stop';\n",
        "    document.body.appendChild(recordButton);\n",
        "\n",
        "    var handleSuccess = function(stream) {\n",
        "        gumStream = stream;\n",
        "        var options = {\n",
        "            mimeType: 'audio/webm;codecs=opus'\n",
        "        };\n",
        "        recorder = new MediaRecorder(stream, options);\n",
        "        recorder.ondataavailable = function(e) {\n",
        "            var url = URL.createObjectURL(e.data);\n",
        "            var preview = document.createElement('audio');\n",
        "            preview.controls = true;\n",
        "            preview.src = url;\n",
        "            document.body.appendChild(preview);\n",
        "\n",
        "            reader = new FileReader();\n",
        "            reader.readAsDataURL(e.data);\n",
        "            reader.onloadend = function() {\n",
        "                base64data = reader.result;\n",
        "            }\n",
        "        };\n",
        "        recorder.start();\n",
        "    };\n",
        "\n",
        "    recordButton.onclick = function() {\n",
        "        if (recorder && recorder.state == \"recording\") {\n",
        "            recorder.stop();\n",
        "            gumStream.getAudioTracks()[0].stop();\n",
        "            recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "        }\n",
        "    };\n",
        "\n",
        "    navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "}\n",
        "\n",
        "startRecording();\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Initialize SpaCy models for multiple languages\n",
        "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "    nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "    nlp_es = spacy.load(\"es_core_news_sm\")\n",
        "    nlp_hi = spacy.load(\"xx_ent_wiki_sm\")  # Using the Hindi model for illustration\n",
        "\n",
        "    # Translate input to English using OpenAI\n",
        "    translated_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "    # Apply SpaCy NER to extract locations in English\n",
        "    doc_en = nlp_en(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc_en.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # If no locations found in English, try French\n",
        "    if not locations:\n",
        "        doc_fr = nlp_fr(translated_input)\n",
        "        for ent in doc_fr.ents:\n",
        "            if ent.label_ == \"LOC\":\n",
        "                locations.append(ent.text)\n",
        "\n",
        "    # If still no locations found, try Spanish\n",
        "    if not locations:\n",
        "        doc_es = nlp_es(translated_input)\n",
        "        for ent in doc_es.ents:\n",
        "            if ent.label_ == \"LOC\":\n",
        "                locations.append(ent.text)\n",
        "\n",
        "    # If still no locations found, try Hindi\n",
        "    if not locations:\n",
        "        doc_hi = nlp_hi(translated_input)\n",
        "        for ent in doc_hi.ents:\n",
        "            if ent.label_ == \"LOC\":\n",
        "                locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Wait for audio recording to complete\n",
        "            print(\"Speak your request. Recording will stop automatically.\")\n",
        "            eval_js(\"new Promise((resolve, reject) => setTimeout(() => resolve(base64data), 30000));\")\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            # Convert audio to text\n",
        "            user_input = \" \".join([word.capitalize() for word in transcribe_audio(audio).split()])\n",
        "\n",
        "        else:\n",
        "            # Handle text input directly\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "        # Handle language specification (optional)\n",
        "        language_code = None\n",
        "        if \":\" in user_input:\n",
        "            parts = user_input.split(\":\")\n",
        "            user_input = parts[0].strip()\n",
        "            language_code = parts[1].strip().lower()\n",
        "\n",
        "        if language_code:\n",
        "            # Translate user input to English if necessary\n",
        "            user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "        # Extract locations from user input\n",
        "        start_location, end_location = extract_locations(user_input)\n",
        "        if not start_location or not end_location:\n",
        "            print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        # Generate route description\n",
        "        route_description = generate_route_description(start_location, end_location)\n",
        "        print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "CJ6D1BI41iZE",
        "outputId": "98515de0-9291-43fe-ce4d-1b28daf1141b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: audio\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "\n",
              "function startRecording() {\n",
              "    var recordButton = document.createElement('button');\n",
              "    recordButton.innerText = 'Recording... Click to stop';\n",
              "    document.body.appendChild(recordButton);\n",
              "\n",
              "    var handleSuccess = function(stream) {\n",
              "        gumStream = stream;\n",
              "        var options = {\n",
              "            mimeType: 'audio/webm;codecs=opus'\n",
              "        };            \n",
              "        recorder = new MediaRecorder(stream, options);\n",
              "        recorder.ondataavailable = function(e) {            \n",
              "            var url = URL.createObjectURL(e.data);\n",
              "            var preview = document.createElement('audio');\n",
              "            preview.controls = true;\n",
              "            preview.src = url;\n",
              "            document.body.appendChild(preview);\n",
              "\n",
              "            reader = new FileReader();\n",
              "            reader.readAsDataURL(e.data); \n",
              "            reader.onloadend = function() {\n",
              "                base64data = reader.result;\n",
              "            }\n",
              "        };\n",
              "        recorder.start();\n",
              "    };\n",
              "\n",
              "    recordButton.onclick = function() {\n",
              "        if (recorder && recorder.state == \"recording\") {\n",
              "            recorder.stop();\n",
              "            gumStream.getAudioTracks()[0].stop();\n",
              "            recordButton.innerText = \"Saving the recording... Please wait!\";\n",
              "        }\n",
              "    };\n",
              "\n",
              "    navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "}\n",
              "\n",
              "startRecording();\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speak your request. Recording will stop automatically.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9c7f43495bdd>\u001b[0m in \u001b[0;36m<cell line: 207>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-9c7f43495bdd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# Evaluate JavaScript to get base64 encoded audio data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base64data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Convert from webm to wav format using ffmpeg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from langchain.pipeline import Pipeline\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize langchain pipeline for translation\n",
        "pipeline = Pipeline(source_lang=\"auto\", target_lang=\"en\")\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Translate input to English using langchain\n",
        "    translated_input = pipeline.translate(user_input)\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp_en(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            # Translate audio to text using OpenAI Whisper\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            # Handle language specification (optional)\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                # Translate user input to English if necessary\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            # Extract locations from user input\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "tw8WjA133Sar",
        "outputId": "c76d9d04-ebdd-4184-f65d-ad05ae457870"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e216869652ba>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mffmpeg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.pipeline'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    doc = nlp_en(user_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcE7CF2j3-5Y",
        "outputId": "cc0179bd-d87f-4f9d-e324-995f897c917e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: from kozhikode to kannur\n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input: Take me from kozhikode to kannur\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: Take me from kozhikode to kannur\n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input: exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] =\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = user_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Iterate over words to find locations\n",
        "    for i in range(len(words)):\n",
        "        if words[i] in direction_keywords:\n",
        "            if i + 1 < len(words):\n",
        "                if start_location is None:\n",
        "                    start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                elif end_location is None:\n",
        "                    end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "5qUng4LJ4VsL",
        "outputId": "fa8f7008-2697-4ecf-8075-87e1094a4ea2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: take me from kannur to kozhikode\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: take me from kannur to kozhikode\n",
            "Route Description: Sure! Here's a detailed driving route from Me (assuming starting point) to Kannur in Kerala:\n",
            "\n",
            "1. Start from your location in Me.\n",
            "2. Head southeast on Me Bypass Rd.\n",
            "3. Turn left onto NH66/National Highway 66.\n",
            "4. Continue on NH66, passing through towns like Thalassery and Mahe.\n",
            "5. Once you reach Thalassery, stay on NH66 and continue towards Kannur.\n",
            "6. Pass by landmarks such as Dharmadam and Kannadikkal.\n",
            "7. Keep following NH66 until you reach your destination in Kannur.\n",
            "\n",
            "Distance: The approximate driving distance between Me and Kannur is around 75-80 kilometers, depending on your exact starting point.\n",
            "\n",
            "Please note that road conditions and traffic may vary, so I recommend checking a live traffic update or using a GPS navigation app for real-time directions during your journey.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: TRANSLATE THE ABOVE IN HINDI\n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input:         \n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ea0ec72bb332>\u001b[0m in \u001b[0;36m<cell line: 188>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-ea0ec72bb332>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_models = {\n",
        "    \"en\": spacy.load(\"en_core_web_sm\"),\n",
        "    \"fr\": spacy.blank(\"fr\"),  # French\n",
        "    \"es\": spacy.blank(\"es\"),  # Spanish\n",
        "    \"hi\": spacy.blank(\"xx\")   # Hindi (Note: xx doesn't have an actual model, it's a placeholder)\n",
        "}\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = user_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Check if an appropriate SpaCy model is available for the input language\n",
        "    if input_language in nlp_models:\n",
        "        nlp = nlp_models[input_language]\n",
        "        doc = nlp(user_input)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"LOC\":\n",
        "                if start_location is None:\n",
        "                    start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "                elif end_location is None:\n",
        "                    end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any(char >= '' and char <= '' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "ErCsgBI86HbY",
        "outputId": "a20d90e1-f8b4-407e-9f51-9a3ffc54fd1e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d66fa04a97f2>\u001b[0m in \u001b[0;36m<cell line: 209>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-d66fa04a97f2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "xxNk0fuw6hXH",
        "outputId": "dad4b932-2cbb-4606-bd74-dc85ddca64c4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Route Description: Sure, I can provide you with a detailed driving route from Me to Kozhikode in Kerala, India. Please note that the route provided is based on a general path and may vary based on current road conditions and traffic.\n",
            "\n",
            "Starting Point: Me (Assuming current location)\n",
            "Destination: Kozhikode\n",
            "\n",
            "Route:\n",
            "1. Start by heading south on [Your current location road] towards [Main Road].\n",
            "2. Merge onto [Main Road] and continue straight for about 5 kilometers.\n",
            "3. Take a slight left onto [Highway/Interstate] and drive for approximately 20 kilometers.\n",
            "4. Continue on [Highway/Interstate] until you reach [Landmark 1] on your right-hand side. This marks the halfway point of your journey.\n",
            "5. Keep driving straight on [Highway/Interstate] for another 40 kilometers.\n",
            "6. Take the exit towards [Kozhikode City] and merge onto [Local Road].\n",
            "7. Follow the signs towards Kozhikode and drive for approximately 15 kilometers.\n",
            "8. You will enter Kozhikode city limits. Continue straight on [Main Road] for about 5 kilometers.\n",
            "9. Turn left onto [Kozhikode City Center Road].\n",
            "10. Continue for another 2 kilometers until you reach your destination in Kozhikode.\n",
            "\n",
            "Total Distance: Approximately 87 kilometers\n",
            "Estimated Driving Time: About 2 hours (depending on traffic conditions)\n",
            "\n",
            "Please note that the provided directions are for guidance purposes, and it is advisable to use GPS navigation or maps for real-time updates and the most accurate route information. Safe travels!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-76623eb65f7e>\u001b[0m in \u001b[0;36m<cell line: 209>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-76623eb65f7e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    return input(\"Please enter your current location: \").strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-xPvYANv7Fvf",
        "outputId": "b9b1c69e-b799-4607-8961-319528e2d759"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: NIT CALICUT\n",
            "Route Description: Driving route from NIT CALICUT to Kozhikode:\n",
            "\n",
            "1. Start at NIT CALICUT, Head south toward NIT Road - 200m\n",
            "2. Turn left onto NIT Road - 500m\n",
            "3. Continue onto National Highway 766/NH766 - 3.5km\n",
            "4. Continue straight to stay on NH766 - 6.5km\n",
            "5. Turn left onto Vadakara Bypass Rd in Kakkattil - 280m\n",
            "6. Continue onto Kuttiady - Thamarassery - 26.7km\n",
            "7. Turn right onto SH34 - 13.7km\n",
            "8. Continue onto Calicut - Waynad Road/SH34 - 19.2km\n",
            "9. Turn left onto Mavoor Road - 750m\n",
            "10. Continue straight onto Stadium Puthiyara Rd - 500m\n",
            "11. Continue onto Mavoor Rd - 1.2km\n",
            "12. Turn right onto Arayidathupalam Rd - 350m\n",
            "13. Turn left onto Francis Rd - 200m\n",
            "14. Continue onto IG Road - 300m\n",
            "15. Continue straight to stay on IG Road - 160m\n",
            "16. Turn right onto Kannur Rd/SH38 - 350m\n",
            "17. Turn left onto Wayanad Rd/SH38 - 1.1km\n",
            "18. Turn right onto Mananchira Rd - 240m\n",
            "19. Turn left at the 1st cross street onto Mavoor Rd - 850m\n",
            "20. Continue onto Mavoor Rd Overbridge - 200m\n",
            "21. Continue onto Arayidathupalam Rd - 650m\n",
            "22. Continue onto Sweet Meat Cross Rd - 450m\n",
            "23. Continue onto Taluk Kacheri Cross Rd - 150m\n",
            "24. Turn right onto Sethi Asilamadam - 220m\n",
            "25. Continue onto Parking Area - Destination will be on the right.\n",
            "\n",
            "Total distance: Approximately 84 km\n",
            "Estimated driving time: Around 2 hours 15 minutes\n",
            "\n",
            "Landmarks along the way: NIT CALICUT, Vadakara Bypass Rd, Kuttiady, Thamarassery, Mavoor Road, Calicut Waynad Road, Mavoor Rd Overbridge, Arayidathupalam Rd, Mananchira Rd, Sweet Meat Cross Rd, Taluk Kacheri Cross Rd\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ab1fa1b49f8c>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-ab1fa1b49f8c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"fr\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    return input(\"Please enter your current location: \").strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiA3asqc7rdT",
        "outputId": "0477282b-fe12-4479-e125-c052c016ba98"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: Paragon \n",
            "Route Description: I'm sorry, but I cannot provide real-time driving directions as the route may change due to traffic conditions or road closures. \n",
            "\n",
            "However, I can suggest a general route from Paragon to Kozhikode for you to use as a reference:\n",
            "\n",
            "1. Start at Paragon and head north on the main road.\n",
            "2. Continue straight for about 3 kilometers until you reach a roundabout.\n",
            "3. Take the second exit at the roundabout to continue on the main road.\n",
            "4. Stay on the main road for approximately 10 kilometers, passing through various neighborhoods and smaller towns.\n",
            "5. After this, you will reach a larger town where you will need to make a right turn onto a major highway leading towards Kozhikode.\n",
            "6. Follow the highway signs towards Kozhikode, which is approximately 50 kilometers away.\n",
            "7. As you near Kozhikode, follow the signs to the city center or desired destination within Kozhikode.\n",
            "\n",
            "Remember to use GPS navigation or a mapping app for real-time updates on the best route to take based on current traffic conditions. Drive safely and enjoy your journey to Kozhikode!\n",
            "Enter 'text' for text input or 'audio' for audio input:         \n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input: exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] =\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"fr\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    current_location = input(\"Please enter your current location: \").strip()\n",
        "    language_choice = input(\"Enter 'en' for English or 'fr' for French for the route description: \").strip().lower()\n",
        "    return current_location, language_choice\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'fr':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'fr':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G12_TXYs9cDt",
        "outputId": "058b37f9-be3e-4720-e78e-c72cf8d6ae6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:         \n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter 'text' for text input or 'audio' for audio input:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: NIT CALCIUT\n",
            "Enter 'en' for English or 'fr' for French for the route description: fr\n",
            "Route Description:\n",
            "Bien sr, voici une voie de conduite dtaille de NIT Calicut  Kozhikode:\n",
            "\n",
            "1. Commencez au campus NIT Calicut et dirigez-vous vers l'est sur Nit Road.\n",
            "2. Tournez  gauche sur NH 66 (Kannur Kozhikode-Mangalore Highway).\n",
            "3. Continuez sur NH 66 sur environ 25 km.\n",
            "4. Passez Thikkodi et continuez le NH 66.\n",
            "5. Aprs environ 15 km, vous atteindrez Feroke.\n",
            "6. Continuez sur NH 66 via Feroke et vers Kozhikode.\n",
            "7. Aprs environ 5 km, vous atteindrez les limites de la ville de Kozhikode.\n",
            "8. Suivez les panneaux pour votre destination spcifique au sein de Kozhikode pour d'autres instructions.\n",
            "\n",
            "Cobines importantes:\n",
            "- Campus NIT Calicut: point de dpart\n",
            "- Thikkodi\n",
            "- Feroke\n",
            "- Limites de la ville de Kozhikode\n",
            "\n",
            "Distance:\n",
            "La distance totale en voiture de NIT Calicut  Kozhikode est d'environ 45 km, et cela devrait prendre environ 1  1,5 heure pour atteindre votre destination en fonction des conditions de circulation.\n",
            "Enter 'text' for text input or 'audio' for audio input: exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    current_location = input(\"Please enter your current location: \").strip()\n",
        "    language_choice = input(\"Enter 'en' for English or 'fr' for French for the route description: \").strip().lower()\n",
        "    return current_location, language_choice\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'fr':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'fr':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRlm470d_Aee",
        "outputId": "cec19bff-a6db-43c1-d09c-43370f78d66a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: NIT CALICUT\n",
            "Enter 'en' for English or 'fr' for French for the route description: fr\n",
            "Route Description:\n",
            "Bien sr!Voici une route de conduite dtaille de NIT Calicut  Kozhikode:\n",
            "\n",
            "1. Commencez  NIT Calicut, National Institute of Technology Calicut, Kozhikode, Kerala 673601.\n",
            "\n",
            "2. Dirigez-vous vers le sud sur Nit Road vers la 3e Avenue.\n",
            "\n",
            "3. Tournez  gauche sur la 3e Avenue.\n",
            "\n",
            "4. Continuez directement pour rester sur la 3e Avenue.\n",
            "\n",
            "5. Tournez  gauche sur Nellikode - Kuthiravattom Road.\n",
            "\n",
            "6. Continuez directement sur Nellikode Road.\n",
            "\n",
            "7. Tournez  droite sur Wayanad Road.\n",
            "\n",
            "8. Continuez directement sur Kozhikode Bypass Road.\n",
            "\n",
            "9. Suivez Kozhikode Bypass Road sur environ 14 km.\n",
            "\n",
            "10. Prenez la sortie vers Perambra.\n",
            "\n",
            "11. Mergez-vous sur Perambra - Kozhikode Road.\n",
            "\n",
            "12. Continuez sur Perambra - Kozhikode Road sur environ 5 km.\n",
            "\n",
            "13. Continuez sur la route nationale 66.\n",
            "\n",
            "14. Continuez sur NH 66 pour encore 10 km jusqu' ce que vous atteigniez la ville de Kozhikode.\n",
            "\n",
            "15. Une fois  Kozhikode, vous pouvez naviguer sur la base de monuments spcifiques ou de votre destination finale dans la ville.\n",
            "\n",
            "Veuillez noter que l'itinraire rel peut varier en fonction des conditions de circulation, des fermetures de routes et d'autres facteurs, c'est donc toujours une bonne ide d'avoir un GPS ou une carte  porte de main pendant le voyage.Voyages srs!\n",
            "Enter 'text' for text input or 'audio' for audio input: exiy\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: exit\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    current_location = input(\"Please enter your current location: \").strip()\n",
        "    language_choice = input(\"Enter 'en' for English or 'fr' for French for the route description: \").strip().lower()\n",
        "    return current_location, language_choice\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'hi':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'fr':\n",
        "                route_description = translator.translate(route_description, src='en', dest='fr').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "Cn3y4oeJ_T1Y",
        "outputId": "b379514b-76cb-4edd-f790-03148bb3dc9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b2261cf327c6>\u001b[0m in \u001b[0;36m<cell line: 239>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-b2261cf327c6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mcurrent_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# Assuming the current location is needed for accurate route description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b2261cf327c6>\u001b[0m in \u001b[0;36mget_current_location\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_current_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mcurrent_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter your current location: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mlanguage_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'en' for English or 'fr' for French for the route description: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def get_current_location():\n",
        "    current_location = input(\"Please enter your current location: \").strip()\n",
        "    language_choice = input(\"Enter 'en' for English or 'hi' for Hindi for the route description: \").strip().lower()\n",
        "    return current_location, language_choice\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'hi':\n",
        "                route_description = translator.translate(route_description, src='en', dest='hi').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "\n",
        "            # Translate route description if needed\n",
        "            if language_choice == 'hi':\n",
        "                route_description = translator.translate(route_description, src='en', dest='hi').text\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g-B9MpEQ_fqq",
        "outputId": "450543ab-5cd7-4aee-eb59-87cb45773ad0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: Kozhikode city mall\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: en\n",
            "Route Description:\n",
            "Sure! Here is a detailed driving route from Kozhikode City Mall to Kozhikode:\n",
            "\n",
            "1. Start at Kozhikode City Mall and head south on Mavoor Road towards Puthiyapalam Junction.\n",
            "2. Continue straight on Mavoor Road and pass by the Malabar Gold & Diamonds showroom on your left.\n",
            "3. After approximately 1.5 km, you will reach the Puthiyapalam Junction. Take a left turn onto Wayanad Road.\n",
            "4. Continue on Wayanad Road for about 2.7 km until you reach the Arayidathupalam Junction. Turn right onto Y.M.C.A Cross Road.\n",
            "5. Drive straight on Y.M.C.A Cross Road for about 1.2 km and then take a right turn onto Mavoor Road.\n",
            "6. Continue on Mavoor Road for approximately 2.5 km. You will pass by Paragon Restaurant on your left.\n",
            "7. Keep going straight on Mavoor Road until you reach the Kozhikode Bypass Junction. Take a right turn here.\n",
            "8. Follow the Kozhikode Bypass Road for about 3.6 km until you reach the Eranhikkal Junction. Turn left onto the National Highway 66 (NH66).\n",
            "9. Drive on NH66 for about 3.0 km. You will pass by Kozhikode Medical College on your left.\n",
            "10. Continue on NH66 and after approximately 2.8 km, you will reach the Arayidathupalam Junction. Turn right onto the Mavoor Road.\n",
            "11. Drive straight on Mavoor Road for about 1.2 km until you reach the Mavoor Road Junction. Take a left turn here.\n",
            "12. Drive for another 1.8 km on Mavoor Road until you reach the K.S.R.T.C Bus Stand on your left.\n",
            "13. From here, continue straight on Mavoor Road for about 1.3 km until you reach Kozhikode Beach.\n",
            "14. You have now reached Kozhikode city.\n",
            "\n",
            "Total approximate distance: 22.8 km\n",
            "\n",
            "This route will take you through major roads and landmarks, making it easier for you to navigate from Kozhikode City Mall to Kozhikode.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO KANNUR in detail\n",
            "Please enter your current location: nit calicut\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: en\n",
            "Route Description:\n",
            "The route from NIT Calicut to Kozhikode City is fairly straightforward and relatively short. Here is a detailed driving route with major turns, landmarks, and distances:\n",
            "\n",
            "1. Start at NIT Calicut campus in Kozhikode district, Kerala.\n",
            "2. Head southeast on NIT Rd towards Mananchira Rd.\n",
            "3. Turn left onto Mananchira Rd and continue straight.\n",
            "4. Take the first exit at the roundabout onto Kuttichira Rd.\n",
            "5. Continue on Kuttichira Rd and pass by Hotel Paramount Inn on the left.\n",
            "6. Turn left onto S M St/Raman Menon Rd.\n",
            "7. Continue straight on Raman Menon Rd and cross over Mananchira Lake.\n",
            "8. Turn right onto Mavoor Rd and continue for about 13 km.\n",
            "9. Pass by the Pavangad junction and continue on Mavoor Rd.\n",
            "10. After about 2 km, you will reach the Eranjipalam junction.\n",
            "11. Keep left to continue on Mavoor Rd.\n",
            "12. After about 2.5 km, you will reach Arayidathupalam junction.\n",
            "13. Continue straight on Mavoor Rd for another 2 km.\n",
            "14. Turn left onto Wayanad Rd/SH 38.\n",
            "15. Drive for about 1.5 km and you will reach the Palayam junction.\n",
            "16. Continue straight on Wayanad Rd, passing by the Government Engineering College on your left.\n",
            "17. After about 3 km, you will reach the Mini Bypass Rd junction.\n",
            "18. Turn left onto Mini Bypass Rd.\n",
            "19. Continue on Mini Bypass Rd for about 3 km, passing by Malabar Christian College on your left.\n",
            "20. Turn right onto Kozhikode Bypass Rd.\n",
            "21. Continue on Kozhikode Bypass Rd for about 2 km.\n",
            "22. Take the exit towards Kozhikode City/Kappad.\n",
            "23. Merge onto Beach Rd and follow the signs towards Kozhikode City.\n",
            "24. Drive straight on Beach Rd, passing by the Calicut University main gate on your left.\n",
            "25. Continue on Beach Rd for about 4 km, passing by Kozhikode Medical College on your right.\n",
            "26. Drive past the Kozhikode Railway Station on your right and you will reach the heart of Kozhikode City.\n",
            "\n",
            "Congratulations, you have arrived at Kozhikode City! The total driving distance from NIT Calicut to Kozhikode is approximately 20 km and can take around 45 minutes, depending on traffic conditions. Enjoy your journey!\n",
            "Enter 'text' for text input or 'audio' for audio input: TAKE ME FROM KOZHIKODE TO KANNUR in detail\n",
            "Invalid input type. Please enter 'text' or 'audio'.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: TAKE ME FROM KOZHIKODE TO KANNUR in detail\n",
            "Please enter your current location: Kozhikode\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: en\n",
            "Route Description:\n",
            "As Kozhikode is a city in itself, providing a driving route from Kozhikode to Kozhikode would essentially be planning a round trip within the city limits. However, I can still provide you with a general route for a city tour in Kozhikode that covers major landmarks and attractions:\n",
            "\n",
            "1. Start at your location in Kozhikode.\n",
            "2. Head towards Mananchira Square, a historic park in the heart of the city, located approximately 1 km from the center.\n",
            "3. From Mananchira Square, drive towards Kozhikode Beach, a popular spot for locals and tourists, approximately 3 km away.\n",
            "4. Next, visit the Beypore Port, a historic port town known for its ancient shipbuilding industry, which is around 11 km from Kozhikode Beach.\n",
            "5. From Beypore Port, make your way to Tali Shiva Temple, an ancient temple located about 7 km from Beypore.\n",
            "6. Drive towards the Kadalundi Bird Sanctuary, a haven for birdwatchers, which is approximately 23 km from Tali Shiva Temple.\n",
            "7. Finally, return to your starting point in Kozhikode to complete the city tour.\n",
            "\n",
            "Please note that the distances and routes provided are approximate and may vary based on traffic conditions and specific starting points within the city. It's also recommended to use a GPS navigation system or maps for real-time directions while driving in Kozhikode.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: exit\n",
            "Could not extract start and end locations from input. Please try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eb3526bbb64f>\u001b[0m in \u001b[0;36m<cell line: 239>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-eb3526bbb64f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter 'text' for text input or 'audio' for audio input: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language=\"en\"):\n",
        "    if input_language == \"hi\":\n",
        "        # Translate Hindi input to English\n",
        "        translated_input = translator.translate(user_input, dest=\"en\").text\n",
        "    else:\n",
        "        translated_input = user_input\n",
        "\n",
        "    # Define keywords that indicate a direction or route request\n",
        "    direction_keywords = [\"from\", \"to\", \"take\", \"route\", \"drive\"]\n",
        "\n",
        "    # Split user input into words\n",
        "    words = translated_input.lower().split()\n",
        "\n",
        "    # Initialize variables to store start and end locations\n",
        "    start_location = None\n",
        "    end_location = None\n",
        "\n",
        "    # Use English SpaCy model for extraction\n",
        "    doc = nlp_en(translated_input)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            if start_location is None:\n",
        "                start_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "            elif end_location is None:\n",
        "                end_location = ent.text.capitalize()  # Assuming location is in title case\n",
        "\n",
        "    # Fallback to keyword-based extraction for other languages or if no entities found\n",
        "    if not start_location or not end_location:\n",
        "        for i in range(len(words)):\n",
        "            if words[i] in direction_keywords:\n",
        "                if i + 1 < len(words):\n",
        "                    if start_location is None:\n",
        "                        start_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "                    elif end_location is None:\n",
        "                        end_location = words[i + 1].capitalize()  # Assuming location is in title case\n",
        "\n",
        "    return start_location, end_location\n",
        "\n",
        "def generate_route_description(start_location, end_location, language_choice):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    route_description = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Translate route description if needed\n",
        "    if language_choice == 'hi':\n",
        "        route_description = translator.translate(route_description, src='en', dest='hi').text\n",
        "\n",
        "    return route_description\n",
        "\n",
        "def get_current_location():\n",
        "    current_location = input(\"Please enter your current location: \").strip()\n",
        "    language_choice = input(\"Enter 'en' for English or 'hi' for Hindi for the route description: \").strip().lower()\n",
        "    return current_location, language_choice\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any('\\u0900' <= char <= '\\u097f' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location, language_choice)\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, language_code)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            current_location, language_choice = get_current_location()\n",
        "\n",
        "            # Assuming the current location is needed for accurate route description\n",
        "            start_location = current_location\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location, language_choice)\n",
        "\n",
        "            print(\"Route Description:\")\n",
        "            print(route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRKij1nk_R0N",
        "outputId": "d5b7249d-a160-46d9-9a2f-abf8896ecdba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:  TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: Kozhikode\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: hi\n",
            "Route Description:\n",
            "  !             ,                           ,            \n",
            "\n",
            " : \n",
            "\n",
            "1.           \n",
            "2.       \n",
            "3.      \n",
            "4.          \n",
            "5.     \n",
            "6.           \n",
            "7.      \n",
            "8.            \n",
            "9.     \n",
            "10.             \n",
            "11. NH766   \n",
            "12. NH766   15     \n",
            "13. NH766      \n",
            "14.  -     \n",
            "15.      \n",
            "16.  10         \n",
            "17. NH766    \n",
            "18. NH766   10            \n",
            "19.     \n",
            "20.             \n",
            "\n",
            " : \n",
            "\n",
            "          ,   ,                50-60  ,        \n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n",
            "Please enter your current location: KOZHOKODE\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: EN\n",
            "Route Description:\n",
            "As you have mentioned the same origin and destination (KOZHOKODE to Kozhikode), I assume you are looking for a round trip route in Kozhikode. Here is a detailed driving route:\n",
            "\n",
            "1. Start at KOZHOKODE and head south on NH66.\n",
            "2. In about 4 km, you will reach Koyilandy town.\n",
            "3. Continue on NH66 towards Kozhikode city.\n",
            "4. After approximately 26 km, you will arrive at Kozhikode Beach.\n",
            "5. Continue on the same road which will take you to the heart of Kozhikode city.\n",
            "6. Make sure to visit landmarks like Mananchira Square, Calicut Beach, and the Kozhikode Backwaters.\n",
            "7. To return to KOZHOKODE, reverse your route and drive back on NH66.\n",
            "\n",
            "Total Distance: Approximately 60 km (round trip)\n",
            "\n",
            "Please note that this route may vary based on traffic conditions, road closures, and other factors. Make sure to use GPS or maps for real-time navigation while driving. Enjoy your trip!\n",
            "Enter 'text' for text input or 'audio' for audio input: TEXT\n",
            "Enter your request: FROM CHENNAI TO TRICHY\n",
            "Please enter your current location: IIT MADRAS\n",
            "Enter 'en' for English or 'hi' for Hindi for the route description: EN\n",
            "Route Description:\n",
            "Certainly! Here is a detailed driving route from IIT Madras to Trichy:\n",
            "\n",
            "1. Start at IIT Madras, Chennai.\n",
            "2. Take the Rajiv Gandhi Salai/OMR road towards Tambaram.\n",
            "3. Continue on Rajiv Gandhi Salai for about 12 km.\n",
            "4. Turn right onto the Inner Ring Road and continue for another 2 km.\n",
            "5. Merge onto Grand Southern Trunk Road/NH32 and continue for about 20 km.\n",
            "6. Take the left exit towards NH32, then merge onto NH32.\n",
            "7. Continue straight on NH32 for approximately 280 km.\n",
            "8. As you approach Trichy, watch for signs directing you to the city center.\n",
            "9. Follow the directions towards the city center to reach your destination in Trichy.\n",
            "\n",
            "Along the way, you may come across landmarks such as Vandalur Zoo, Chengalpattu Junction, Villupuram Junction, Ulundurpet Junction, and Perambalur. \n",
            "\n",
            "Please note that the travel time may vary depending on traffic conditions and your driving speed. It is recommended to use a navigation app or GPS to assist you during the journey.\n",
            "Enter 'text' for text input or 'audio' for audio input: EXIT\n",
            "Exiting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "oJuzGdX390Yz",
        "outputId": "29be3f32-68b9-467f-938f-36778cdf0e7a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=f4043b67f147e04ace6ee1016788f6974ed07270c8358e96db43bfd2f7e427ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "  Attempting uninstall: googletrans\n",
            "    Found existing installation: googletrans 3.0.0\n",
            "    Uninstalling googletrans-3.0.0:\n",
            "      Successfully uninstalled googletrans-3.0.0\n",
            "Successfully installed googletrans-4.0.0rc1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "googletrans"
                ]
              },
              "id": "176b933c71224caeb18c49e36e8f5cf7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language):\n",
        "    # Translate input to English using Google Translate\n",
        "    translated_input = translator.translate(user_input, dest='en').text\n",
        "\n",
        "    # Initialize SpaCy model for English\n",
        "    nlp = nlp_en if input_language == \"en\" else spacy.blank(\"xx\")  # Using English or blank for other languages\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any(char >= '' and char <= '' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, \"en\")\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "K58Dv9BF42-T",
        "outputId": "db0387d9-4379-48ab-ad00-6c782d0f7a7f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:         \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'group'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e0339f5698f9>\u001b[0m in \u001b[0;36m<cell line: 193>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-e0339f5698f9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_text_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not extract start and end locations from input. Please try again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e0339f5698f9>\u001b[0m in \u001b[0;36mextract_locations\u001b[0;34m(user_input, input_language)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Translate input to English using Google Translate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtranslated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Initialize SpaCy model for English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language):\n",
        "    # Translate input to English using OpenAI\n",
        "    translated_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "    # Initialize SpaCy model for English\n",
        "    nlp = nlp_en if input_language == \"en\" else spacy.blank(\"xx\")  # Using English or blank for other languages\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any(char >= '' and char <= '' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, \"en\")\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "nLKXNQAQ5G0i",
        "outputId": "26293abe-a742-4302-d767-080ceef1a898"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: TAKE ME FROM KOZHIKODE TO CHENNAI in detail\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'Translate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-77c50aaede90>\u001b[0m in \u001b[0;36m<cell line: 180>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-77c50aaede90>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not extract start and end locations from input. Please try again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-77c50aaede90>\u001b[0m in \u001b[0;36mextract_locations\u001b[0;34m(user_input, input_language)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Translate input to English using OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtranslated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_text_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Initialize SpaCy model for English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-77c50aaede90>\u001b[0m in \u001b[0;36mtranslate_text_openai\u001b[0;34m(text, target_language)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_text_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     response = openai.Translate(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         inputs={\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Translate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "from googletrans import Translator\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] \"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "# Initialize SpaCy model for English\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def translate_text_google(text, target_language=\"en\"):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(text, dest=target_language)\n",
        "    return translation.text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input, input_language):\n",
        "    # Translate input to English using Google Translate\n",
        "    translated_input = translate_text_google(user_input, \"en\")\n",
        "\n",
        "    # Initialize SpaCy model for English\n",
        "    nlp = nlp_en if input_language == \"en\" else spacy.blank(\"xx\")  # Using English or blank for other languages\n",
        "\n",
        "    # Apply SpaCy NER to extract locations\n",
        "    doc = nlp(translated_input)\n",
        "    locations = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"LOC\":\n",
        "            locations.append(ent.text)\n",
        "\n",
        "    # Return start and end locations (first two locations found)\n",
        "    if len(locations) >= 2:\n",
        "        return locations[0], locations[1]\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            display(HTML(AUDIO_HTML))\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "            # Detect language of audio input\n",
        "            input_language = \"hi\" if any(char >= '' and char <= '' for char in audio_data) else \"en\"\n",
        "\n",
        "            start_location, end_location = extract_locations(audio_data, input_language)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            start_location, end_location = extract_locations(user_input, \"en\")\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "cZv27fOS5WeP",
        "outputId": "2e9bb96b-fedb-4045-993c-201306073ebd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request: take me from kannur to kozhikode\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'group'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-065b59e0b5d0>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-065b59e0b5d0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not extract start and end locations from input. Please try again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-065b59e0b5d0>\u001b[0m in \u001b[0;36mextract_locations\u001b[0;34m(user_input, input_language)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Translate input to English using Google Translate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mtranslated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_text_google\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Initialize SpaCy model for English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-065b59e0b5d0>\u001b[0m in \u001b[0;36mtranslate_text_google\u001b[0;34m(text, target_language)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_text_google\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install langchain.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v6MqVDp3mbW",
        "outputId": "7e750d5e-ca5b-461e-9d31-59e4fda7d949"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain.pipeline (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain.pipeline\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9T7NjXg3Z3S",
        "outputId": "bbb170ae-27ee-47ef-c08b-9d02cb165c81"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from IPython.display import HTML, display\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import spacy\n",
        "\n",
        "# Set the OpenAI API key (replace with your actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# HTML/JavaScript for recording audio in Colab\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var recordButton = document.createElement('button');\n",
        "recordButton.innerText = 'Press to start recording';\n",
        "document.body.appendChild(recordButton);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "        mimeType: 'audio/webm;codecs=opus'\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    recorder.ondataavailable = function(e) {\n",
        "        var url = URL.createObjectURL(e.data);\n",
        "        var preview = document.createElement('audio');\n",
        "        preview.controls = true;\n",
        "        preview.src = url;\n",
        "        document.body.appendChild(preview);\n",
        "\n",
        "        reader = new FileReader();\n",
        "        reader.readAsDataURL(e.data);\n",
        "        reader.onloadend = function() {\n",
        "            base64data = reader.result;\n",
        "        }\n",
        "    };\n",
        "    recorder.start();\n",
        "};\n",
        "\n",
        "recordButton.onclick = function() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... Please wait!\";\n",
        "    }\n",
        "};\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def translate_text_openai(text, target_language=\"en\"):\n",
        "    response = openai.Translate(\n",
        "        model=\"text-davinci-003\",\n",
        "        inputs={\n",
        "            \"text\": text,\n",
        "            \"to_language\": target_language\n",
        "        }\n",
        "    )\n",
        "    translated_text = response['translations'][0]['translated_text']\n",
        "    return translated_text\n",
        "\n",
        "def translate_audio_to_text(audio_data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": audio_data}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Translate input to English using OpenAI\n",
        "    translated_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "    # Placeholder for location extraction logic (to be implemented)\n",
        "    # Here, you would implement logic to extract locations from translated_input\n",
        "\n",
        "    return None, None  # Replace with actual location extraction logic\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Placeholder for route description generation (to be implemented)\n",
        "    # Here, you would implement logic to generate route description\n",
        "\n",
        "    return \"Route description placeholder\"\n",
        "\n",
        "def main():\n",
        "    print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        input_type = input(\"Enter 'text' for text input or 'audio' for audio input: \").strip().lower()\n",
        "\n",
        "        if input_type == 'exit':\n",
        "            print('Exiting')\n",
        "            break\n",
        "\n",
        "        if input_type not in ['text', 'audio']:\n",
        "            print(\"Invalid input type. Please enter 'text' or 'audio'.\")\n",
        "            continue\n",
        "\n",
        "        if input_type == 'audio':\n",
        "            # Display HTML/JS for recording audio in Colab\n",
        "            display(HTML(AUDIO_HTML))\n",
        "\n",
        "            # Evaluate JavaScript to get base64 encoded audio data\n",
        "            data = eval_js(\"base64data\")\n",
        "            binary = b64decode(data.split(',')[1])\n",
        "\n",
        "            # Convert from webm to wav format using ffmpeg\n",
        "            process = (\n",
        "                ffmpeg.input('pipe:0')\n",
        "                .output('pipe:1', format='wav')\n",
        "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True, input=binary)\n",
        "            )\n",
        "            output, _ = process.communicate()\n",
        "\n",
        "            # Convert the byte stream to a format compatible with scipy.io.wavfile.read\n",
        "            riff_chunk_size = len(output) - 8\n",
        "            q = riff_chunk_size\n",
        "            b = []\n",
        "            for i in range(4):\n",
        "                q, r = divmod(q, 256)\n",
        "                b.append(r)\n",
        "\n",
        "            riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "            # Read wav data\n",
        "            sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "            # Translate audio to text using OpenAI Whisper\n",
        "            audio_data = translate_audio_to_text(audio.tobytes())\n",
        "            print(\"Translated Audio:\", audio_data)\n",
        "\n",
        "        else:\n",
        "            user_input = input(\"Enter your request: \")\n",
        "\n",
        "            # Handle language specification (optional)\n",
        "            language_code = None\n",
        "            if \":\" in user_input:\n",
        "                parts = user_input.split(\":\")\n",
        "                user_input = parts[0].strip()\n",
        "                language_code = parts[1].strip().lower()\n",
        "\n",
        "            if language_code:\n",
        "                # Translate user input to English if necessary\n",
        "                user_input = translate_text_openai(user_input, \"en\")\n",
        "\n",
        "            # Extract locations from user input\n",
        "            start_location, end_location = extract_locations(user_input)\n",
        "            if not start_location or not end_location:\n",
        "                print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Generate route description\n",
        "            route_description = generate_route_description(start_location, end_location)\n",
        "            print(\"Route Description:\", route_description)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "sKxY6S902QpU",
        "outputId": "b5d3ec9a-8079-47dc-ff74-a55107e2fa77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter 'text' for text input or 'audio' for audio input: text\n",
            "Enter your request:      \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'Translate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a710c6bfcb2b>\u001b[0m in \u001b[0;36m<cell line: 168>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-a710c6bfcb2b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Extract locations from user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mstart_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mend_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not extract start and end locations from input. Please try again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a710c6bfcb2b>\u001b[0m in \u001b[0;36mextract_locations\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Translate input to English using OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtranslated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_text_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Placeholder for location extraction logic (to be implemented)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a710c6bfcb2b>\u001b[0m in \u001b[0;36mtranslate_text_openai\u001b[0;34m(text, target_language)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_text_openai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     response = openai.Translate(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         inputs={\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Translate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wadxFPrc0Hq3",
        "outputId": "2801c66a-fffb-4478-95a5-95c6e553a787"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0smVQawHylzH",
        "outputId": "d159c41f-2d03-42a3-8eda-06bd819ec741"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall sounddevice\n",
        "!pip install sounddevice\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fha_kf63xWuT",
        "outputId": "924e6495-4d2c-4b69-d7eb-95880b547943"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: sounddevice 0.4.7\n",
            "Uninstalling sounddevice-0.4.7:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/_sounddevice.py\n",
            "    /usr/local/lib/python3.10/dist-packages/sounddevice-0.4.7.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/sounddevice.py\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled sounddevice-0.4.7\n",
            "Collecting sounddevice\n",
            "  Using cached sounddevice-0.4.7-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
            "Installing collected packages: sounddevice\n",
            "Successfully installed sounddevice-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sounddevice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a90VRxrJxLfs",
        "outputId": "6c1ffd26-c618-4bff-a5dd-603eb3855ae5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.10/dist-packages (0.4.7)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBriWP1NxD5b",
        "outputId": "7b473c27-0467-4d97-e215-834cdeabb955"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper\n",
        "!pip install pyaudio\n",
        "!pip install googletrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X97FLEPrvp7_",
        "outputId": "f2cb3903-e25d-4f7e-98c7-1d173c302cbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Collecting pyaudio\n",
            "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.6.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import re\n",
        "import subprocess\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def extract_locations(user_input):\n",
        "    # Regular expression to extract location names\n",
        "    locations = re.findall(r\"\\b[A-Z][a-z]+\\b\", user_input)  # Matches capitalized words (potential location names)\n",
        "\n",
        "    if len(locations) >= 2:\n",
        "        start_location, end_location = locations[:2]\n",
        "        return start_location, end_location\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def generate_route_description(start_location, end_location):\n",
        "    # Prompt for the OpenAI API\n",
        "    prompt = f\"Provide a detailed driving route from {start_location} to {end_location}. Include major turns, landmarks, and distances.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def record_audio():\n",
        "    # Record audio using the microphone\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Recording audio. Speak now...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio, language='en-US')\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, I could not understand audio.\")\n",
        "        return \"\"\n",
        "    except sr.RequestError:\n",
        "        print(\"Sorry, there was a problem with the speech recognition service.\")\n",
        "        return \"\"\n",
        "\n",
        "# Run the chatbot in a loop\n",
        "print(\"Route Description Chatbot. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    user_input = input(\"Enter your request (text/audio): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        break\n",
        "\n",
        "    if user_input.lower().startswith('audio'):\n",
        "        # Record audio\n",
        "        user_input = record_audio()\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "    start_location, end_location = extract_locations(user_input)\n",
        "    if not start_location or not end_location:\n",
        "        print(\"Could not extract start and end locations from input. Please try again.\")\n",
        "        continue\n",
        "\n",
        "    route_description = generate_route_description(start_location, end_location)\n",
        "    print(\"Route Description:\", route_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Nr15LKylukmy",
        "outputId": "91b3562f-6382-4b9f-a9c7-5cc1ad37ff6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Route Description Chatbot. Type 'exit' to quit.\n",
            "Enter your request (text/audio): Text\n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter your request (text/audio): kannur to kochi\n",
            "Could not extract start and end locations from input. Please try again.\n",
            "Enter your request (text/audio): from kannur to kozhikode\n",
            "Could not extract start and end locations from input. Please try again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c60a23924698>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Route Description Chatbot. Type 'exit' to quit.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your request (text/audio): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moVD-OT1tjvK",
        "outputId": "35206432-8b84-4af2-de74-58abe0dc6e02"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsBn1iJ7s0Nz",
        "outputId": "76f25d52-1636-4058-f050-1269544d56d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OJ5464lr68v",
        "outputId": "f58258ba-b32c-4b37-8858-1f8fecf7f331"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MzjA1TUylGsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = PPOConfig(\n",
        "    model_name=\"lvwerra/gpt2-imdb\",\n",
        "    learning_rate=1.41e-5,\n",
        ")\n",
        "\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
      ],
      "metadata": {
        "id": "LyLgv6NclG1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
        "    customize this function to train the model on its own dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # load imdb with datasets\n",
        "    ds = load_dataset(dataset_name, split=\"train\")\n",
        "    ds = ds.rename_columns({\"text\": \"review\"})\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
        "\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n"
      ],
      "metadata": {
        "id": "3q6BMBKBrHKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}